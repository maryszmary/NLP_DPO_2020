{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_First_Time.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izA3-6kffbdT",
        "colab_type": "text"
      },
      "source": [
        "# Используем BERT впервые\n",
        "\n",
        "Источник: [Jay Alamar](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n",
        "\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png\" />\n",
        "\n",
        "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of sentences from film reviews. And we will calssify each sentence as either speaking \"positively\" about its subject of \"negatively\".\n",
        "\n",
        "## Models: Sentence Sentiment Classification\n",
        "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
        "\n",
        "Under the hood, the model is actually made up of two model.\n",
        "\n",
        "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
        "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n",
        "\n",
        "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n",
        "\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />\n",
        "\n",
        "## Dataset\n",
        "The dataset we will use in this example is [SST2](https://nlp.stanford.edu/sentiment/index.html), which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\n",
        "\n",
        "\n",
        "<table class=\"features-table\">\n",
        "  <tr>\n",
        "    <th class=\"mdc-text-light-green-600\">\n",
        "    sentence\n",
        "    </th>\n",
        "    <th class=\"mdc-text-purple-600\">\n",
        "    label\n",
        "    </th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      1\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      apparently reassembled from the cutting room floor of any given daytime soap\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      0\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      they presume their audience won't sit still for a sociology lesson\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      0\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      1\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
        "      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n",
        "    </td>\n",
        "    <td class=\"mdc-bg-purple-50\">\n",
        "      1\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "## Installing the transformers library\n",
        "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9ENLU90WGl",
        "colab_type": "code",
        "outputId": "d4c958ca-d369-4cf7-8994-299c680599c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 2.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 26.9MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 34.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 39.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 40.4MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 43.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 46.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 48.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 51.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 53.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 53.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 53.6MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 53.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 53.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 53.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 53.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 53.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 53.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 59.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 55.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.43)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=031be510b17bd3a6beb5ebaf6ed325cb06fbdb02b041148d885c2e6e9a28aed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvFvBLJV0Dkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ-42fh0hjsF",
        "colab_type": "text"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyoj29J24hPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8hu_H0-xnrd",
        "colab_type": "code",
        "outputId": "00be51e0-5d85-4e43-dd41-86e8ddb61320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  1\n",
              "0  a stirring , funny and finally transporting re...  1\n",
              "1  apparently reassembled from the cutting room f...  0\n",
              "2  they presume their audience wo n't sit still f...  0\n",
              "3  this is a visually stunning rumination on love...  1\n",
              "4  jonathan parker 's bartleby should have been t...  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2WRyXsUjT3q",
        "colab_type": "code",
        "outputId": "af6cc150-2e45-4ee3-8554-0d97ffdd030c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6920, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMVE3waNhuNj",
        "colab_type": "text"
      },
      "source": [
        "Возьмём первые 2,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTM3hOHW4hUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_1 = df[:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRc2L89hh1Tf",
        "colab_type": "text"
      },
      "source": [
        "Баланс классов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvcfcCP5xpZ",
        "colab_type": "code",
        "outputId": "82596e22-d4dd-480a-8f5d-834e1f238a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "batch_1[1].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1041\n",
              "0     959\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MO08_KiAOb",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Pre-trained BERT model\n",
        "Let's now load a pre-trained BERT model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1InADgf5xm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDBMn3wiSX6",
        "colab_type": "text"
      },
      "source": [
        "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
        "\n",
        "\n",
        "### Токенизация\n",
        "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg82ndBA5xlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized = batch_1[0].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPdPH9Y308-K",
        "colab_type": "code",
        "outputId": "e4ebe23d-b32b-4e0e-aecb-67d32ba5c223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "tokenized[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
              "1    [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
              "2    [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
              "3    [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
              "4    [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzxDwhclkOa5",
        "colab_type": "code",
        "outputId": "60ad4f6d-afef-478b-9dd6-52bcd747e718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df.iloc[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CH0dXL7kU5y",
        "colab_type": "code",
        "outputId": "6c15f212-f9fc-4298-dd72-ca0928ba7266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df.iloc[1][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apparently reassembled from the cutting room floor of any given daytime soap'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-3PZrUokjVD",
        "colab_type": "code",
        "outputId": "04248c2e-16d8-43cf-89bb-27d73c168660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.vocab['[CLS]']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtnFsqawk913",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2token = {ind:tok for tok, ind in tokenizer.vocab.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYqZmOSqlFSv",
        "colab_type": "code",
        "outputId": "8f1aebb9-8a15-48c5-f644-1b5e3faed530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "id2token[101]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35QRHFFKjw2T",
        "colab_type": "code",
        "outputId": "f009a76a-6752-4657-fdc6-e0bc2a60707f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.wordpiece_tokenizer.tokenize('a stirring , funny and finally')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'stirring', ',', 'funny', 'and', 'finally']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kywVGVJklP9W",
        "colab_type": "code",
        "outputId": "2849c840-af72-4277-b4e4-0761a86b05db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.decode([101, 1037, 18385, 1010, 6057, 1998, 2633])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] a stirring, funny and finally'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWpbKM0plrXD",
        "colab_type": "code",
        "outputId": "8786e311-2bdb-4a72-9b46-70bc2fac8a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.wordpiece_tokenizer.tokenize('pythonista')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['python', '##ista']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idWsg0rMmFF7",
        "colab_type": "code",
        "outputId": "6864b532-34fd-470c-a986-5fc16b345b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.wordpiece_tokenizer.tokenize('cowork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cow', '##or', '##k']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwjUwYgi-uL",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
        "\n",
        "### Padding\n",
        "Выравниваем предложения по длине с помощью нулевых токенов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URn-DWJt5xhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# находим самое длинное предложение\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "# заполняем обучающие данные, где не хватает длины до максимума -- добавляем нули\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdi7uXo95xeq",
        "colab_type": "code",
        "outputId": "b75d59f7-e3d5-41ee-931d-7eb4a5371e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 59)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZBsYSDjzDV",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "\n",
        "Теперь создаём отдельную переменную, чтобы сказать берту, что надо игнорировать паддинг при подсчёте attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_iGRNa_Ozc",
        "colab_type": "code",
        "outputId": "07bac877-b402-4b13-f9ca-d4aecb528d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 59)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwX4c8D5nK07",
        "colab_type": "code",
        "outputId": "ab4c3ed7-f952-469b-ab3d-6209cdf402b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(tokenized[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BufTWLdDqR7F",
        "colab_type": "code",
        "outputId": "de167f1e-a1fb-47f4-f6f8-487675b95738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "attention_mask[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAm3aPzsnP2U",
        "colab_type": "code",
        "outputId": "0f634c76-1942-4827-e483-c1ce7e2259bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sum(attention_mask[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-CQB9-kN99",
        "colab_type": "text"
      },
      "source": [
        "## Используем BERT\n",
        "\n",
        "Функция `model()` прогоняет предложения через BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39UVjAV56PJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoCep_WVuB3v",
        "colab_type": "text"
      },
      "source": [
        "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
        "\n",
        "Берём оттуда только представления первого токена -- `[CLS]`. Это представление и будет нашими признаками."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYlDYCz7oGPO",
        "colab_type": "code",
        "outputId": "b573bf73-ea93-4d5a-ad97-05976185618b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "last_hidden_states[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2000, 59, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9t60At16PVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTIhZduUodsY",
        "colab_type": "code",
        "outputId": "a156f447-de51-4204-b483-8415d7d5746e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSWDlrqUok4d",
        "colab_type": "code",
        "outputId": "7205bb6c-7c73-43cb-ab89-2edbd624cb44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(features[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZVU66Gurr-",
        "colab_type": "text"
      },
      "source": [
        "Метки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3fX2yh6PTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = batch_1[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoEvM2evRx1",
        "colab_type": "text"
      },
      "source": [
        "## LogReg на фичах из BERT\n",
        "Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddAqbkoU6PP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCT9u8vAwnID",
        "colab_type": "text"
      },
      "source": [
        "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. `LogisticRegression(C=5.2)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-EVWx4CzBc",
        "colab_type": "code",
        "outputId": "86eb68fa-ab0b-4ff4-8957-bba4229f6084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rUMKuVgwzkY",
        "colab_type": "text"
      },
      "source": [
        "## Оцениваем результат\n",
        "Accuracy на тесте:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoyxRJ7ECTA",
        "colab_type": "code",
        "outputId": "5d859899-d9e6-46e0-a522-48bd8ff2d685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDytulAEo2lY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h1HwmAXo6ag",
        "colab_type": "code",
        "outputId": "f4ecfb41-97f9-4b3f-90c7-5d8d4689785b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(classification_report(lr_clf.predict(test_features), test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       245\n",
            "           1       0.85      0.83      0.84       255\n",
            "\n",
            "    accuracy                           0.84       500\n",
            "   macro avg       0.84      0.84      0.84       500\n",
            "weighted avg       0.84      0.84      0.84       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75oyhr3VxHoE",
        "colab_type": "text"
      },
      "source": [
        "Сравним с DummyClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnwgmqNG7i5l",
        "colab_type": "code",
        "outputId": "ba62d139-4e3f-442a-e755-31b6d05b6399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dummy classifier score: 0.526 (+/- 0.03)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lg4LOpoxSOR",
        "colab_type": "text"
      },
      "source": [
        "А что насчёт луших классификаторов?\n",
        "\n",
        "## Proper SST2 scores\n",
        "For reference, the [highest accuracy score](http://nlpprogress.com/english/sentiment_analysis.html) for this dataset is currently **96.8**. DistilBERT can be trained to improve its score on this task – a process called **fine-tuning** which updates BERT’s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of **90.7**. The full size BERT model achieves **94.9**.\n",
        "\n",
        "\n",
        "\n",
        "And that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at [fine-tuning](https://huggingface.co/transformers/examples.html#glue). You can also go back and switch from distilBERT to BERT and see how that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQuqV6cnWQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkMAund-E2-o",
        "colab_type": "text"
      },
      "source": [
        "### Как использовать BERT в качестве эмбеддингов слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saaKUzQ-Bi1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 = 'wall street'\n",
        "s2 = 'i broke the wall'\n",
        "s3 = 'another brick in the wall'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-nCj94BthT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1_tok = tokenizer.encode(s1)\n",
        "s2_tok = tokenizer.encode(s2)\n",
        "s3_tok = tokenizer.encode(s3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvHcW-FPqq6j",
        "colab_type": "code",
        "outputId": "b179ac30-04a3-45bb-c455-7420fce8f12f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.vocab['wall']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2813"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMlGWGGgClKC",
        "colab_type": "code",
        "outputId": "c6c0541b-e0e4-4ee7-ba17-9c7803057682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s1_tok.index(2813)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrHnETOhDCkF",
        "colab_type": "code",
        "outputId": "29e5018f-305a-4101-c5d0-849e130bfafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s2_tok.index(2813)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBJI9UEkqmaK",
        "colab_type": "code",
        "outputId": "c3771bb8-c4ca-4023-c0d4-e271c46e5af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "s3_tok.index(2813)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGiMsr34B9Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    last_hidden_states1 = model(torch.tensor([s1_tok]))\n",
        "    last_hidden_states2 = model(torch.tensor([s2_tok]))\n",
        "    last_hidden_states3 = model(torch.tensor([s3_tok]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6VwsFz0DacM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bank1_emb = last_hidden_states1[0][:,1,:]\n",
        "bank2_emb = last_hidden_states2[0][:,4,:]\n",
        "bank3_emb = last_hidden_states3[0][:,5,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ287iydEsGu",
        "colab_type": "text"
      },
      "source": [
        "Теперь посчитаем расстояние:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVki0iW0EWIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2MySmk9Eaax",
        "colab_type": "code",
        "outputId": "81586360-1a69-4213-fde5-e827d3d30afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cosine_similarity(bank1_emb.numpy(), bank2_emb.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4556053]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxBT-BRdEyGk",
        "colab_type": "code",
        "outputId": "24d20cba-5e69-4431-fed4-c9032e71ea79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cosine_similarity(bank1_emb.numpy(), bank3_emb.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.45279628]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_UaspKPEzWx",
        "colab_type": "code",
        "outputId": "03f099e6-8dee-4b46-897d-88820aa0a465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cosine_similarity(bank2_emb.numpy(), bank3_emb.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.86112154]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4w9HcNSuoUX",
        "colab_type": "text"
      },
      "source": [
        "### Для IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh5Ncm-yuf7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt0YAigGunLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YcY2XLvzl_",
        "colab_type": "text"
      },
      "source": [
        "## Русский -- DeepPavlov"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgoR8STSv1Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = ppb.AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akYc247Kv1to",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ppb.AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1mY1rTpwG9b",
        "colab_type": "code",
        "outputId": "7bd74c5d-75fd-46b1-e574-e1fba5ebc6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.wordpiece_tokenizer.tokenize('Русский текст')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Русский', 'текст']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Law4gpUQx8Jj",
        "colab_type": "code",
        "outputId": "ef7dc63e-9e36-4d0b-bb90-578f32438a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.wordpiece_tokenizer.tokenize('печенюшка')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['печен', '##юшка']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3XF4jZyw7Zm",
        "colab_type": "text"
      },
      "source": [
        "#### Классификация ленты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7v8-wo_xA6Q",
        "colab_type": "code",
        "outputId": "bb6fe923-4491-4f08-9d82-f73b9c544c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "! wget -O lenta-ru-train.csv https://www.dropbox.com/s/kdupcw1llbdbqwl/lenta-ru-train.csv?dl=0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 14:43:03--  https://www.dropbox.com/s/kdupcw1llbdbqwl/lenta-ru-train.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601f:1::a27d:901\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/kdupcw1llbdbqwl/lenta-ru-train.csv [following]\n",
            "--2020-04-28 14:43:03--  https://www.dropbox.com/s/raw/kdupcw1llbdbqwl/lenta-ru-train.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucba45f3711b72420da6972c239c.dl.dropboxusercontent.com/cd/0/inline/A2v1kRRFFjc8fjnUli6aBkQC19tmXEExB1Mrr2RnTgUq_rTThXWt1zq0gRGGoPpT1uvqC33tanB7_REgEVPcj-y8vNC404XymQjau-OsvpTSEzo6PvmsdQOP_nva7xtBK-U/file# [following]\n",
            "--2020-04-28 14:43:04--  https://ucba45f3711b72420da6972c239c.dl.dropboxusercontent.com/cd/0/inline/A2v1kRRFFjc8fjnUli6aBkQC19tmXEExB1Mrr2RnTgUq_rTThXWt1zq0gRGGoPpT1uvqC33tanB7_REgEVPcj-y8vNC404XymQjau-OsvpTSEzo6PvmsdQOP_nva7xtBK-U/file\n",
            "Resolving ucba45f3711b72420da6972c239c.dl.dropboxusercontent.com (ucba45f3711b72420da6972c239c.dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to ucba45f3711b72420da6972c239c.dl.dropboxusercontent.com (ucba45f3711b72420da6972c239c.dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 517902316 (494M) [text/plain]\n",
            "Saving to: ‘lenta-ru-train.csv’\n",
            "\n",
            "lenta-ru-train.csv  100%[===================>] 493.91M  31.0MB/s    in 16s     \n",
            "\n",
            "2020-04-28 14:43:21 (30.3 MB/s) - ‘lenta-ru-train.csv’ saved [517902316/517902316]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6ilhgEExLwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lenta = pd.read_csv('lenta-ru-train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0jV4UZOxm_b",
        "colab_type": "code",
        "outputId": "d703305e-84a4-40d8-dedb-b88ebf5536ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df_lenta.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>topic_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Инфляция в январе 2006 года составит 2,6 процента</td>\n",
              "      <td>Глава Росстата Владимир Соколин заявил, что в ...</td>\n",
              "      <td>Экономика</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Никита Михалков учредил День российского кино</td>\n",
              "      <td>У российских кинематографистов  появится новый...</td>\n",
              "      <td>Культура</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Марко Матерацци вернется в строй к матчу с ЦСКА</td>\n",
              "      <td>Медицинский штаб миланского \"Интера\" обнародов...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Определены лауреаты премии \"Книга года\"</td>\n",
              "      <td>Премии \"Книга года\" в 13 номинациях вручены на...</td>\n",
              "      <td>Культура</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Гол Роналду со штрафного спас португальцев от ...</td>\n",
              "      <td>Сборная Португалии сыграла вничью с командой И...</td>\n",
              "      <td>Спорт</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... topic_label\n",
              "0  Инфляция в январе 2006 года составит 2,6 процента  ...           0\n",
              "1      Никита Михалков учредил День российского кино  ...           3\n",
              "2    Марко Матерацци вернется в строй к матчу с ЦСКА  ...           1\n",
              "3            Определены лауреаты премии \"Книга года\"  ...           3\n",
              "4  Гол Роналду со штрафного спас португальцев от ...  ...           1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMi0eEkgZDFz",
        "colab_type": "text"
      },
      "source": [
        "#### Задание: написать извлечение эмбеддингов из BERT для графы title\n",
        "\n",
        "NB: rubert выбаёт **очень** большие эмбеддинги, их стоит обрезать, а данные -- поделить на кусочки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z0iKqsuYu8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIt_lip0Yvvc",
        "colab_type": "text"
      },
      "source": [
        "#### Классифицируем в sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl-84qFX0xyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, df_lenta_batch.topic[:2000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gESJ1f7x0rPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEZgO1Z5Q1EA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "73a5949b-76de-489b-c132-c10e51d40e03"
      },
      "source": [
        "lr.fit(train_features, train_labels)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n5IX15YRHMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "100cc2d2-6213-4851-a8f8-a8eb7de317aa"
      },
      "source": [
        "print(classification_report(lr.predict(test_features), test_labels))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Бизнес       0.17      0.17      0.17        12\n",
            "       Культура       0.68      0.80      0.74        97\n",
            "Наука и техника       0.79      0.77      0.78       104\n",
            "          Спорт       0.88      0.83      0.85       143\n",
            "      Экономика       0.80      0.76      0.78       144\n",
            "\n",
            "       accuracy                           0.78       500\n",
            "      macro avg       0.66      0.67      0.66       500\n",
            "   weighted avg       0.78      0.78      0.78       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CWFsE8IZht_",
        "colab_type": "text"
      },
      "source": [
        "#### Альтернативное задание: IMDB \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOzjF59yZyXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/devkosal/fastai_roberta/master/fastai_roberta_imdb/imdb_dataset.csv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}